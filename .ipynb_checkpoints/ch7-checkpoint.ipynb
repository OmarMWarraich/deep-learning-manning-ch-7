{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Deep Learning Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.1 Introduction to the functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, layers\n",
    "input_tensor = Input (shape=(32,))     # A tensor\n",
    "dense = layers.Dense(32, activation = 'relu')   # A layer is a function\n",
    "output_tensor = dense(input_tensor)         # A layer may be called a tensor and it returns a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimal Eg., Sequential Model with its eq in functional API side by side:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "\n",
    "seq_model = Sequential()       # Seq Md\n",
    "seq_model.add(layers.Dense(32, activation='relu', input_shape=(64,)))\n",
    "seq_model.add(layers.Dense(32, activation='relu'))\n",
    "seq_model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "input_tensor = Input(shape=(64,))\n",
    "x = layers.Dense(32, activation='relu')(input_tensor)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)    # Func eq\n",
    "\n",
    "model = Model(input_tensor, output_tensor)   # The Model class turns an input tensor and output tensor in2 a model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The APi is same as that of sequntial when it comes to compiling training or evaluating such an instance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')  # Compiles the model\n",
    "\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 64))\n",
    "y_train = np.random.random((1000, 10))         # Gen dummy numpy data to train on\n",
    "\n",
    "model.fit(x_train, y_train,epochs=10, batch_size=128)     # Trains the model for 10 epochs\n",
    "score = model.evaluate(x_train, y_train)       # evaluates the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-input models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functional API helps build models having multiple inputs. Use Keras merge operation such as keras.layers.add, keras.layers.concatenate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Functional API implwemetation of a two-point question-answering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "\n",
    "text_vocabulary_size = 10000\n",
    "question_vocabulary_size = 10000\n",
    "answer_vocabulary_size = 500\n",
    "\n",
    "text_input = Input(shape=(None,), dtype = 'int32', name = 'text')     #The text input is a variable-length sequence\n",
    "                                                                     #of integers. Note: U can optionally name the inputs.\n",
    "embedded_text = layers.Embedding(         # Embeds the inputs into a sequence of vectors of size 64.                \n",
    "    64, text_vocabulary_size)(text_input)    \n",
    "\n",
    "encoded_text = layers.LSTM(32)(embedded_text)  #Encodes the vectors in a single vector via an LSTM.\n",
    "\n",
    "question_input = Input(shape=(None,),     # Same process (with different layer instances) for the question.\n",
    "                              dtype='int32',\n",
    "                              name='question')\n",
    "\n",
    "embedded_question = layers.Embedding(32,  question_vocabulary_size)(question_input)\n",
    "\n",
    "encoded_question= layers.LSTM(16)(embedded_question)   \n",
    "\n",
    "concatenated = layers.concatenate([encoded_text, encoded_question], axis = -1) #Concatenation\n",
    "\n",
    "answer = layers.Dense(answer_vocabulary_size, activation='softmax')(concatenated) # Adds a softmax classifier on top.\n",
    "\n",
    "model = Model([text_input, question_input], answer) # Specify the two inputs and outputs at model instantiation.\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 7.2  Feeding Data to a multi-input model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples = 1000\n",
    "max_length = 100\n",
    "\n",
    "text = np.random.randint(1, text_vocabulary_size,\n",
    "                         size=(num_samples, max_length))      # Generates dummy Numpy data\n",
    "\n",
    "question = np.random.randint(1, question_vocabulary_size,\n",
    "                         size=(num_samples, max_length))\n",
    "\n",
    "answers = np.random.randint(0, 1, \n",
    "                         size=(num_samples, answer_vocabulary_size))  # Answers are 1hot encoded, not integers.\n",
    "\n",
    "model.fit([text, question], answers, epochs=10, batch_size=128)  # Fitting using a list of inputs.\n",
    "\n",
    "model.fit({'text': text, 'question': question}, answers,\n",
    "          epochs=10, batch_size=128)    # Fitting using a dictionary of inputs(only if inputs are named)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
